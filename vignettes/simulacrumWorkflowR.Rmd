---
title: "Example: Running the whole workflow"
author: "Jakob Skelmose, Lars Nielsen, Charles Vesteghem, Jennifer Bartell, Martin Bøgsted, Rasmus Rask"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example Running the whole workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Load the libraries 
```{r setup}
library(simulacrumWorkflowR)
library(survival)
library(openxlsx)
library(broom)
library(dplyr)
library(dbplyr)
```

# Start measuring the time of the analysis
```{r starting time for logging }
start <- start_time()
```

# Read the dataset
```{r}
dir <- system.file("extdata", "minisimulacrum", package = "simulacrumWorkflowR")
data_frames_lists <- read_simulacrum(dir, selected_files = c("sim_av_patient", "sim_av_tumour")) 
```
A warning is issued after dataframes are loaded, suggesting that dataframe variables should be named consistently with the original database table names. While not strictly necessary for the package’s core functionalities, significant gains are offered by this approach.

Original naming of these dataframes are to simplify the process for NDRS analysts. This alignment of dataframe names with static database table names allows queries within the workflow to be more easily understood. Confusion and additional work in the limited time of the free tier can be caused by diverging from these original names.

Also, a merging function `av_patient_tumour_merge()` depends on the dataframes being named after the original table names. 

# dfs
```{r}
SIM_AV_PATIENT <- data_frames_lists$sim_av_patient
SIM_AV_TUMOUR <- data_frames_lists$sim_av_tumour
```

# demonstration of the sql query translator 
```{r}
query2 <- "select *
from SIM_AV_PATIENT
limit 500;"
```

# Alternatively, make your data management which dplyr, if preferred and then use the dbplyr "show_query()" to show the query needed for the workflow: 
```{r}
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
copy_to(con, SIM_AV_PATIENT, "patients",
  )

patient_db <- tbl(con, "patients")

query_plan_limit <- patient_db %>%
  head(n = 500)
  
show_query(query_plan_limit)
DBI::dbDisconnect(con)
```
To figure our what queries to include for the workflow are dbplyr a great resource to write R code directly to the sqlite database. After the data mangement code have been written, it can be transformed into a sql query with the `show_query()`. The query can be tested in the query_sql function, or be directly incerted into the `create_workflow()`

# Run query on SQLite database
```{r}
query_result <- query_sql("SELECT 
SIM_AV_PATIENT.PATIENTID, SIM_AV_PATIENT.GENDER, SIM_AV_PATIENT.VITALSTATUS, SIM_AV_PATIENT.VITALSTATUSDATE, SIM_AV_TUMOUR.DIAGNOSISDATEBEST, SIM_AV_TUMOUR.AGE, SIM_AV_TUMOUR.PERFORMANCESTATUS, SIM_AV_TUMOUR.SITE_ICD10_O2_3CHAR

FROM SIM_AV_PATIENT
INNER JOIN SIM_AV_TUMOUR 
    ON SIM_AV_PATIENT.patientid = SIM_AV_TUMOUR.patientid;
")
```
The warning message regarding `query_result`, appears because it is the default variable name for database extraction in `create_workflow`. This message should encourage users to create variable names consistent with the workflow, which would minimize time spend for NDRS analysts on adjusting the workflow for the CAS database.

# Another case of dbplyr for data management to produce the necessary queries for the workflow
```{r}
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
copy_to(con, SIM_AV_PATIENT, "SIM_AV_PATIENT", temporary = TRUE, overwrite = TRUE)
copy_to(con, SIM_AV_TUMOUR, "SIM_AV_TUMOUR", temporary = TRUE, overwrite = TRUE)

patient_db <- tbl(con, "SIM_AV_PATIENT")
tumour_db <- tbl(con, "SIM_AV_TUMOUR")

joined_data <- patient_db %>%
    inner_join(tumour_db, by = "PATIENTID")

query_plan_join_select <- joined_data %>%
  select(
      PATIENTID,
      GENDER.x,   
      VITALSTATUS,
      VITALSTATUSDATE,
      DIAGNOSISDATEBEST,
      AGE,
      PERFORMANCESTATUS,
      SITE_ICD10_O2_3CHAR
  )

show_query(query_plan_join_select)

DBI::dbDisconnect(con)
```


# Additional preprocessing
```{r}
df_surv <- survival_days(query_result)
df_surv$VITALSTATUS <- ifelse(df_surv$VITALSTATUS == "A", 1,
                              ifelse(df_surv$VITALSTATUS == "D", 0, NA))

df_complete <- cancer_grouping(df_surv)
extended_summary(df_complete)
```


# example of a Cox model of the synthetic data
```{r}
cox_model <- coxph(Surv(diff_date, VITALSTATUS) ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=df_complete)
summary(cox_model)
```

# Example of a Logistic regression of the synthetic data
```{r}
log_model <- glm(VITALSTATUS ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=df_complete, family = "binomial")
summary(log_model)
```

# Save models as HTML files 
```{r}
output_dir <- "./Outputs"
create_dir(output_dir)

cox_model_sum <- tidy(cox_model)
write.xlsx(cox_model_sum, file.path(output_dir, "cox_model.xlsx"))

log_model_sum <- tidy(log_model)
write.xlsx(log_model_sum, file.path(output_dir, "log_model.xlsx"))
```

# Measure the time it takes to run the analysis to see if the time are within the 3 hours threshold
```{r}
stop <- stop_time()
compute_time_limit(start, stop)
```

As mentioned in the manual of the `compute_time_limit` should the user of this package use this function as a support tool to decide the complexity of the workflow which are intended to be executed on the real CAS data. The users of this package should account for the run time, the complexity and potential adjustment a NDRS analyst would have to account for, and for the difference in compute power of the users machine compared to the machines at NHS. 

# create a full workflow with the analysis code
```{r}
create_workflow(
  libraries = '              
library(survival)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(simulacrumWorkflowR)
',
  query = "                  
SELECT 
SIM_AV_PATIENT.PATIENTID, SIM_AV_PATIENT.GENDER, SIM_AV_PATIENT.VITALSTATUS, SIM_AV_PATIENT.VITALSTATUSDATE, SIM_AV_TUMOUR.DIAGNOSISDATEBEST, SIM_AV_TUMOUR.AGE, SIM_AV_TUMOUR.PERFORMANCESTATUS, SIM_AV_TUMOUR.SITE_ICD10_O2_3CHAR

FROM SIM_AV_PATIENT
INNER JOIN SIM_AV_TUMOUR 
    ON SIM_AV_PATIENT.patientid = SIM_AV_TUMOUR.patientid;",
  data_management = '        
data <- survival_days(query_result)

data_copy <- data

data_copy$VITALSTATUS <- ifelse(data_copy$VITALSTATUS == "A", 1,
                              ifelse(data_copy$VITALSTATUS == "D", 0, NA))
',
  analysis = '
cox_model <- coxph(Surv(diff_date, VITALSTATUS) ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=data_copy)
log_model <- glm(VITALSTATUS ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=data_copy, family = "binomial")
',
  model_results = '
cox_model_sum <- tidy(cox_model)
write.xlsx(cox_model_sum, "cox_model.xlsx")

log_model_sum <- tidy(log_model) 
write.xlsx(log_model_sum, "log_model.xlsx")

')

```

The `create_workflow` function demonstrates the structure of a workflow file. Code components are inserted as strings, and logical order is maintained. A complete workflow's appearance is illustrated without a database setup. If code is inserted in the logical sequence guided by the function, the resulting workflow will be executable on the CAS database.
