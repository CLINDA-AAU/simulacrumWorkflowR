---
title: "Example: Running the whole workflow"
author: "Jakob Skelmose, Lars Nielsen, Charles Vesteghem, Jennifer Bartell, Martin BÃ¸gsted, Rasmus Rask"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example Running the whole workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Start measuring the time of the analysis
```{r starting time for logging }
start <- start_time()
```

# Load the libraries 
```{r setup}
library(simulacrumWorkflowR)
library(survival)
library(openxlsx)
library(broom)
library(dplyr)
library(dbplyr)
```

# Read the dataset
```{r}
dir <- system.file("extdata", "minisimulacrum", package = "simulacrumWorkflowR")
data_frames_lists <- read_simulacrum(dir, selected_files = c("sim_av_patient", "sim_av_tumour")) 
```


# dfs
```{r}
SIM_AV_PATIENT <- data_frames_lists$sim_av_patient
SIM_AV_TUMOUR <- data_frames_lists$sim_av_tumour
```

# demonstration of the sql query translator 
```{r}
query2 <- "select *
from SIM_AV_PATIENT
limit 500;"
```

# Alternatively, make your data management which dplyr, if preferred and then use the dbplyr "show_query()" to show the query needed for the workflow: 
```{r}
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
copy_to(con, SIM_AV_PATIENT, "patients",
  )

patient_db <- tbl(con, "patients")

query_plan_limit <- patient_db %>%
  head(n = 500)
  
show_query(query_plan_limit)
DBI::dbDisconnect(con)
```
# To figure our what queries to include for the workflow are dbplyr a great resource to write R code directly to the sqlite database. After the data mangement code have been written, it can be transformed into a sql query with the `show_query()`. The query can be tested in the query_sql function, or be directly incerted into the `create_workflow()`

# Run query on SQLite database
```{r}
df1 <- query_sql("SELECT 
SIM_AV_PATIENT.PATIENTID, SIM_AV_PATIENT.GENDER, SIM_AV_PATIENT.VITALSTATUS, SIM_AV_PATIENT.VITALSTATUSDATE, SIM_AV_TUMOUR.DIAGNOSISDATEBEST, SIM_AV_TUMOUR.AGE, SIM_AV_TUMOUR.PERFORMANCESTATUS, SIM_AV_TUMOUR.SITE_ICD10_O2_3CHAR

FROM SIM_AV_PATIENT
INNER JOIN SIM_AV_TUMOUR 
    ON SIM_AV_PATIENT.patientid = SIM_AV_TUMOUR.patientid;
")
```

# Another case of dbplyr for data management to produce the necessary queries for the workflow
```{r}
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
copy_to(con, SIM_AV_PATIENT, "SIM_AV_PATIENT", temporary = TRUE, overwrite = TRUE)
copy_to(con, SIM_AV_TUMOUR, "SIM_AV_TUMOUR", temporary = TRUE, overwrite = TRUE)

patient_db <- tbl(con, "SIM_AV_PATIENT")
tumour_db <- tbl(con, "SIM_AV_TUMOUR")

joined_data <- patient_db %>%
    inner_join(tumour_db, by = "PATIENTID")

query_plan_join_select <- joined_data %>%
  select(
      PATIENTID,
      GENDER.x,   
      VITALSTATUS,
      VITALSTATUSDATE,
      DIAGNOSISDATEBEST,
      AGE,
      PERFORMANCESTATUS,
      SITE_ICD10_O2_3CHAR
  )

show_query(query_plan_join_select)

show_query(query_plan_join_select)

DBI::dbDisconnect(con)
```


# Additional preprocessing
```{r}
df_surv <- survival_days(df1)
df_surv$VITALSTATUS <- ifelse(df_surv$VITALSTATUS == "A", 1,
                              ifelse(df_surv$VITALSTATUS == "D", 0, NA))

df_complete <- cancer_grouping(df_surv)
extended_summary(df_complete)
```


# example of a Cox model of the synthetic data
```{r}
cox_model <- coxph(Surv(diff_date, VITALSTATUS) ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=df_complete)
summary(cox_model)
```

# Example of a Logistic regression of the synthetic data
```{r}
log_model <- glm(VITALSTATUS ~ AGE + factor(GENDER) + factor(PERFORMANCESTATUS), data=df_complete, family = "binomial")
summary(log_model)
```

# Save models as HTML files 
```{r}
output_dir <- "./Outputs"
create_dir(output_dir)

cox_model_sum <- tidy(cox_model)
write.xlsx(cox_model_sum, file.path(output_dir, "cox_model.xlsx"))

log_model_sum <- tidy(log_model)
write.xlsx(log_model_sum, file.path(output_dir, "log_model.xlsx"))
```

# Measure the time it takes to run the analysis to see if the time are within the 3 hours threshold
```{r}
stop <- stop_time()
compute_time_limit(start, stop)
```

# create a full workflow with the analysis code
```{r}
create_workflow(
  libraries = '              
library(survival)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
',
  query = "                  
SELECT *
FROM sim_av_patient
INNER JOIN sim_av_tumour ON sim_av_patient.patientid = sim_av_tumour.patientid;",
  data_management = '        
df2 <- survival_days(df1)

df21 <- df2

df21$VITALSTATUS <- ifelse(df21$VITALSTATUS == "A", 1,
                              ifelse(df21$VITALSTATUS == "D", 0, NA))
',
  analysis = '
cox_model <- coxph(Surv(diff_date, VITALSTATUS) ~ AGE + factor(GENDER.x) + factor(PERFORMANCESTATUS), data=df21)
log_model <- glm(VITALSTATUS ~ AGE + factor(GENDER.x) + factor(PERFORMANCESTATUS), data=df21, family = "binomial")
',
  model_results = '
cox_model_sum <- tidy(cox_model)
write.xlsx(cox_model_sum, "cox_model.xlsx")

log_model_sum <- tidy(log_model) 
write.xlsx(log_model_sum, "log_model.xlsx")

')

```


```{r}
stop <- stop_time()
compute_time_limit(start, stop)
```
